date start : 03 july 2020


10.0 Disclaimer
---------------------------------------



10.1 Process Creation
---------------------------------------
Via fork


10.2 Multi Threading and 1 CPU
---------------------------------------
Yes



10.3 Critical Section
---------------------------------------
Solution


Producer code
Line 3: uses “if” instead of “while”
Line 4: condition_wait doesn’t specify a mutex
Line 7: since only 1 item is added, no need to broadcast, should signal instead
Line 8: missing the mutex_unlock


Consumer code
Line 4: condition_wait doesn’t specify a mutex
Line 7: condition_signal signals the wrong variable, should be signaling not_full
Line 8: missing the mutex_unlock operation


10.4 Calendar Critical Section
---------------------------------------
//Read operation
Lock(&m)


10.5 Signals
---------------------------------------
Recall that all signals are intercepted by a user-level threading library handler, and the user-level threading library installs a handler. This handler determines which user-level thread, if any, the signal be delivered to, and then it takes the appropriate steps to deliver the signal.


10.6 Solaris Papers
---------------------------------------
Process:
1. List of kernel level threads
2. User level registers
3. Virtual Address Space
4. Signal handlers


LWP:
1. User-level registers
2. System call arguments
3. Resource Usage Info


Kernel-threads:
1. Kernel-level registers
2. Scheduling info


CPU:
1. Current thread
2. List of kernel-level threads


10.7 Pipeline Model
---------------------------------------
1. Threads should be allocated as follows:Stage 1 should have 1 thread. This 1 thread will parse a new request every 10ms
Stage 2 should have 3 threads. The requests parsed by Stage 1 get passed to the threads in Stage 2. Each thread picks up a request and needs 30ms to process the image. Hence, we need 3 threads in order to pick up a new request as soon as Stage 1 passes it.
Stage 3 should have 2 threads. This is because Stage 2 will process an image and pass a request every 10ms (once the pipeline is filled). In this way, each Stage 3 thread will pick up a request and send an image in 20ms. Once the pipeline is filled, Stage 3 will be able to pick up a request and send the image every 10ms.


2. The first request will take 60ms. The last stage will continue delivering the remaining 99 requests at 10ms intervals. So, the total is 60 + (99 * 10ms) = 1050ms = 1.05s


3. 100 req / 1.05 sec = 95.2 req/s

10.8 Performance Observations
---------------------------------------
In both cases the dataset will likely fit in cache, but Flash incurs an overhead on each request because Flash must first check for cache residency. In the SPED model, this check is not performed.

When data is present in the cache, there is no need for slow disk I/O operations. Adding threads or processes just adds context switching overheads, but there is no benefit of “hiding I/O latency”.

10.9
---------------------------------------


10.10 Sample Midterm Solutions
---------------------------------------



