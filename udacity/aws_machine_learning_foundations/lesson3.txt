date start : 21 july 2021


3.1 Machine Learning with AWS
---------------------------------------
The AWS achine learning mission is to put machine learning in the hands of every developer.

AWS offers the broadest and deepest set of artificial intelligence (AI) and machine learning (ML) services with unmatched flexibility.
You can accelerate your adoption of machine learning with AWS SageMaker. Models that previously took months to build and required specialized expertise can now be built in weeks or even days.
AWS offers the most comprehensive cloud offering optimized for machine learning.
More machine learning happens at AWS than anywhere else.




3.2 Lesson Overview
---------------------------------------


3.3 AWS Account Requirements
---------------------------------------
AWS Account Requirements
An AWS account is required
To complete the exercises in this course, you need an AWS Account ID.

To set up a new AWS Account ID, follow the directions in How do I create and activate a new Amazon Web Services account?

You are required to provide a payment method when you create the account. To learn about which services are available at no cost, see the AWS Free Tier documentation.

Will these exercises cost anything?
This lesson contains many demos and exercises. You do not need to purchase any AWS devices to complete the lesson. However, please carefully read the following list of AWS services you may need in order to follow the demos and complete the exercises.

Train your computer vision model with AWS DeepLens (optional)
To train and deploy custom models to AWS DeepLens, you use Amazon SageMaker. Amazon SageMaker is a separate service and has its own service pricing and billing tier. It's not required to train a model for this course. If you're interested in training a custom model, please note that it incurs a cost. To learn more about SageMaker costs, see the Amazon SageMaker Pricing.
Train your reinforcement learning model with AWS DeepRacer
To get started with AWS DeepRacer, you receive 10 free hours to train or evaluate models and 5GB of free storage during your first month. This is enough to train your first time-trial model, evaluate it, tune it, and then enter it into the AWS DeepRacer League. This offer is valid for 30 days after you have used the service for the first time.
Beyond 10 hours of training and evaluation, you pay for training, evaluating, and storing your machine learning models. Charges are based on the amount of time you train and evaluate a new model and the size of the model stored. To learn more about AWS DeepRacer pricing, see the AWS DeepRacer Pricing
Generate music using AWS DeepComposer
To get started, AWS DeepComposer provides a 12-month Free Tier for first-time users. With the Free Tier, you can perform up to 500 inference jobs translating to 500 pieces of music using the AWS DeepComposer Music studio. You can use one of these instances to complete the exercise at no cost. To learn more about AWS DeepComposer costs, see the AWS DeepComposer Pricing.
Build a custom generative AI model (GAN) using Amazon SageMaker (optional)
Amazon SageMaker is a separate service and has its own service pricing and billing tier. To train the custom generative AI model, the instructor uses an instance type that is not covered in the Amazon SageMaker free tier. If you want to code along with the instructor and train your own custom model, you may incur a cost. Please note, that creating your own custom model is completely optional. You are not required to do this exercise to complete the course. To learn more about SageMaker costs, see the Amazon SageMaker Pricing.


3.4 Computer Vision and Its Applications
---------------------------------------
How computer vision got started
Early applications of computer vision needed hand-annotated images to successfully train a model.
These early applications had limited applications because of the human labor required to annotate images.

Three main components of neural networks
Input Layer: This layer receives data during training and when inference is performed after the model has been trained.
Hidden Layer: This layer finds important features in the input data that have predictive power based on the labels provided during training.
Output Layer: This layer generates the output or prediction of your model.

Modern computer vision
Modern-day applications of computer vision use neural networks call convolutional neural networks or CNNs.
In these neural networks, the hidden layers are used to extract different information about images. We call this process feature extraction.
These models can be trained much faster on millions of images and generate a better prediction than earlier models.

How this growth occured
Since 2010, we have seen a rapid decrease in the computational costs required to train the complex neural networks used in computer vision.
Larger and larger pre-labeled datasets have become generally available. This has decreased the time required to collect the data needed to train many models.



Image classification is the most common application of computer vision in use today. Image classification can be used to answer questions like What's in this image? This type of task has applications in text detection or optical character recognition (OCR) and content moderation.
Object detection is closely related to image classification, but it allows users to gather more granular detail about an image. For example, rather than just knowing whether an object is present in an image, a user might want to know if there are multiple instances of the same object present in an image, or if objects from different classes appear in the same image.
Semantic segmentation is another common application of computer vision that takes a pixel-by-pixel approach. Instead of just identifying whether an object is present or not, it tries to identify down the pixel level which part of the image is part of the object.
Activity recognition is an application of computer vision that is based around videos rather than just images. Video has the added dimension of time and, therefore, models are able to detect changes that occur over time


New Terms
Input Layer: The first layer in a neural network. This layer receives all data that passes through the neural network.
Hidden Layer: A layer that occurs between the output and input layers. Hidden layers are tailored to a specific task.
Output Layer: The last layer in a neural network. This layer is where the predictions are generated based on the information captured in the hidden layers.


https://www.awsdeeplens.recipes/
https://aws.amazon.com/blogs/machine-learning/
https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/aws-deeplens/
https://docs.aws.amazon.com/deeplens/latest/dg/deeplens-getting-started.html


3.5
---------------------------------------
AWS DeepLens
AWS DeepLens allows you to create and deploy end-to-end computer vision–based applications. The following video provides a brief introduction to how AWS DeepLens works and how it uses other AWS services.

AWS DeepLens is a deep learning–enabled camera that allows you to deploy trained models directly to the device. You can either use sample templates and recipes or train your own model.

The AWS DeepLens device
The AWS DeepLens camera is powered by an Intel® Atom processor, which can process 100 billion floating-point operations per second (GFLOPS). This gives you all the computing power you need to perform inference on your device. The micro HDMI display port, audio out, and USB ports allow you to attach peripherals, so you can get creative with your computer vision applications.

You can use AWS DeepLens as soon as you register it.

How AWS DeepLens works
AWS DeepLens is integrated with multiple AWS services. You use these services to create, train, and launch your AWS DeepLens project. You can think of an AWS DeepLens project as being divided into two different streams as the image shown above.

First, you use the AWS console to create your project, store your data, and train your model.
Then, you use your trained model on the AWS DeepLens device. On the device, the video stream from the camera is processed, inference is performed, and the output from inference is passed into two output streams:
Device stream – The video stream passed through without processing.
Project stream – The results of the model's processing of the video frames.


3.6 A sample project with aws deeplens
---------------------------------------
Machine Learning workflow review
The machine learning workflow contains several steps first introduced in Lesson 2. Let's briefly review the different steps and how they relate to the AWS DeepLens project.

1. Define the problem.
Using machine learning, we want to improve how trash is sorted. We're going to identify objects using a video stream, so we identify this as a computer vision–based problem.
We have access to data that already contains the labels, so we classify this as a supervised learning task.

2. Build the dataset.
Data is essential to any machine learning or computer vision–based project. Before going out and collecting lots of data, we investigate what kinds of data already exist and if they can be used for our application.
In this case, we have the data already collected and labeled.

3. Train the model.
Now that we have our data secured for this project, we use Amazon SageMaker to train our model. We cover specifics about this process in the demonstration video.

4. Evaluate the model.
Model training algorithms use loss functions to bring the model closer to its goals. The exact loss function and related details are outside the scope of this class, but the process is the same.
The loss function improves how well the model detects the different class images (compost, recycling, and landfill) while the model is being trained.

5. Use the model.
We deploy our trained model to our AWS DeepLens device, where inference is performed locally.



3.7 Quiz: Computer Vision
---------------------------------------
object deteciton , Great job! Object detection can be used to not only count the number of objects in an image but also to identify different classes present in an image.

image classification

Semantic (instance) segmentation : Nice job! Yes! Semantic segmentation is best used to identify the exact location (pixel) of an object in an image.

instance (semantic) segmentation

object detection

object detection , semantic (instance) segmentation

image classification


3.8 Reinforcement Learning and Its Applications
--------------------------------------
In reinforcement learning (RL), an agent is trained to achieve a goal based on the feedback it receives as it interacts with an environment. It collects a number as a reward for each action it takes. Actions that help the agent achieve its goal are incentivized with higher numbers. Unhelpful actions result in a low reward or no reward.

With a learning objective of maximizing total cumulative reward, over time, the agent learns, through trial and error, to map gainful actions to situations. The better trained the agent, the more efficiently it chooses actions that accomplish its goal.

Reinforcement learning is used in a variety of fields to solve real-world problems. It’s particularly useful for addressing sequential problems with long-term goals. Let’s take a look at some examples.

RL is great at playing games:
Go (board game) was mastered by the AlphaGo Zero software.
Atari classic video games are commonly used as a learning tool for creating and testing RL software.
StarCraft II, the real-time strategy video game, was mastered by the AlphaStar software.
RL is used in video game level design:
Video game level design determines how complex each stage of a game is and directly affects how boring, frustrating, or fun it is to play that game.
Video game companies create an agent that plays the game over and over again to collect data that can be visualized on graphs.
This visual data gives designers a quick way to assess how easy or difficult it is for a player to make progress, which enables them to find that “just right” balance between boredom and frustration faster.
RL is used in wind energy optimization:
RL models can also be used to power robotics in physical devices.
When multiple turbines work together in a wind farm, the turbines in the front, which receive the wind first, can cause poor wind conditions for the turbines behind them. This is called wake turbulence and it reduces the amount of energy that is captured and converted into electrical power.
Wind energy organizations around the world use reinforcement learning to test solutions. Their models respond to changing wind conditions by changing the angle of the turbine blades. When the upstream turbines slow down it helps the downstream turbines capture more energy.
Other examples of real-world RL include:
Industrial robotics
Fraud detection
Stock trading
Autonomous driving

Agent: The piece of software you are training is called an agent. It makes decisions in an environment to reach a goal.
Environment: The environment is the surrounding area with which the agent interacts.
Reward: Feedback is given to an agent for each action it takes in a given state. This feedback is a numerical reward.
Action: For every state, an agent needs to take an action toward achieving its goal.


3.9 Reinforcement Learning with AWS DeepRacer
---------------------------------------
Agent
The piece of software you are training is called an agent.
It makes decisions in an environment to reach a goal.
In AWS DeepRacer, the agent is the AWS DeepRacer car and its goal is to finish * laps around the track as fast as it can while, in some cases, avoiding obstacles.

Environment
The environment is the surrounding area within which our agent interacts.
For AWS DeepRacer, this is a track in our simulator or in real life.

State
The state is defined by the current position within the environment that is visible, or known, to an agent.
In AWS DeepRacer’s case, each state is an image captured by its camera.
The car’s initial state is the starting line of the track and its terminal state is when the car finishes a lap, bumps into an obstacle, or drives off the track.

Action
For every state, an agent needs to take an action toward achieving its goal.
An AWS DeepRacer car approaching a turn can choose to accelerate or brake and turn left, right, or go straight.

Reward
Feedback is given to an agent for each action it takes in a given state.
This feedback is a numerical reward.
A reward function is an incentive plan that assigns scores as rewards to different zones on the track.

Episode
An episode represents a period of trial and error when an agent makes decisions and gets feedback from its environment.
For AWS DeepRacer, an episode begins at the initial state, when the car leaves the starting position, and ends at the terminal state, when it finishes a lap, bumps into an obstacle, or drives off the track.



An algorithm is a set of instructions that tells a computer what to do. ML is special because it enables computers to learn without being explicitly programmed to do so.

The training algorithm defines your model’s learning objective, which is to maximize total cumulative reward. Different algorithms have different strategies for going about this.
A soft actor critic (SAC) embraces exploration and is data-efficient, but can lack stability.
A proximal policy optimization (PPO) is stable but data-hungry.

An action space is the set of all valid actions, or choices, available to an agent as it interacts with an environment.
Discrete action space represents all of an agent's possible actions for each state in a finite set of steering angle and throttle value combinations.
Continuous action space allows the agent to select an action from a range of values that you define for each sta te.

Hyperparameters are variables that control the performance of your agent during training. There is a variety of different categories with which to experiment. Change the values to increase or decrease the influence of different parts of your model.
For example, the learning rate is a hyperparameter that controls how many new experiences are counted in learning at each step. A higher learning rate results in faster training but may reduce the model’s quality.

The reward function's purpose is to encourage the agent to reach its goal. Figuring out how to reward which actions is one of your most important jobs.

Key points to remember about exploration versus exploitation:

When a car first starts out, it explores by wandering in random directions. However, the more training an agent gets, the more it learns about an environment. This experience helps it become more confident about the actions it chooses.
Exploitation means the car begins to exploit or use information from previous experiences to help it reach its goal. Different training algorithms utilize exploration and exploitation differently.
Key points to remember about the reward graph:

While training your car in the AWS DeepRacer console, your training metrics are displayed on a reward graph.
Plotting the total reward from each episode allows you to see how the model performs over time. The more reward your car gets, the better your model performs.
Key points to remember about AWS DeepRacer:

AWS DeepRacer is a combination of a physical car and a virtual simulator in the AWS Console, the AWS DeepRacer League, and community races.
An AWS DeepRacer device is not required to start learning: you can start now in the AWS console. The 3D simulator in the AWS console is where training and evaluation take place.
New Terms
Exploration versus exploitation: An agent should exploit known information from previous experiences to achieve higher cumulative rewards, but it also needs to explore to gain new experiences that can be used in choosing the best actions in the future.

3.10 Demo: Reinforcement Learning with AWS DeepRacer
---------------------------------------
To get you started with AWS DeepRacer, you receive 10 free hours to train or evaluate models and 5GB of free storage during your first month. This offer is valid for 30 days after you have used the service for the first time. Beyond 10 hours of training and evaluation, you pay for training, evaluating, and storing your machine learning models. Please read the AWS account requirements page for more information.

This demonstration introduces you to the AWS DeepRacer console and walks you through how to use it to build your first reinforcement learning model. You'll use your knowledge of basic reinforcement learning concepts and terminology to make choices about your model. In addition, you'll learn about the following features of the AWS DeepRacer service:

Pro and Open Leagues
Digital rewards
Racer profile
Garage
Sensor configuration
Race types
Time trial
Object avoidance
Head-to-head

This demonstration walks you through the training process in the AWS DeepRacer console. You've learned about:

The reward graph
The training video

This demonstration walks the evaluation process in the AWS DeepRacer console.

Once you've created a successful model, you'll learn how to enter it into a race for the chance to win awards, prizes, and the opportunity to compete in the worldwide AWS DeepRacer Championship.



3.11 Quiz: Reinforcement Learning
---------------------------------------
supervised learning
Correct! In supervised learning, every data has a corresponding label. Reinforcement learning and unsupervised learning models do not need labeled data.

The piece of software you are training that makes decisions in an environment to
reach a goal

false
Correct! This statement is false. "Exploration" is when an agent wanders to discover what actions lead to what feedback in the form of digital rewards. "Exploitation" is using experience to decide.

The more an agent learns about its environment, the more confident it becomes about the actions it chooses.
If an agent doesn't explore enough, it often sticks to information its already learned even if this knowledge doesn't help the agent achieve its goal.
The agent can use information from previous experiences to help it make future decisions that enable it to reach its goal.

state: the current position within the environment that is visible, or known, to
an agent.

action: for every state, an agent needs to do this toward achieving its goal.

episode: represents a period of trial and error when an agent makes decisions
and gets feedback from its environment.

reward: feedback given to an agent for each action it takes in a given state.

environment: the surrounding area our agent interacts with.


3.12 AWS DeepRacer Reinforcement Learning Exercise
---------------------------------------
Exercise: Interpret the reward graph of your first AWS DeepRacer model
Instructions
Train a model in the AWS DeepRacer console and interpret its reward graph.

Part 1: Train a reinforcement learning model using the AWS DeepRacer console
Practice the knowledge you've learned by training your first reinforcement learning model using the AWS DeepRacer console.

If this is your first time using AWS DeepRacer, choose Get started from the service landing page, or choose Get started with reinforcement learning from the main navigation pane.
On the Get started with reinforcement learning page, under Step 2: Create a model and race, choose Create model. Alternatively, on the AWS DeepRacer home page, choose Your models from the main navigation pane to open the Your models page. On the Your models page, choose Create model.
On the Create model page, under Environment simulation, choose a track as a virtual environment to train your AWS DeepRacer agent. Then, choose Next. For your first run, choose a track with a simple shape and smooth turns. In later iterations, you can choose more complex tracks to progressively improve your models. To train a model for a particular racing event, choose the track most similar to the event track.
On the Create model page, choose Next.
On the Create Model page, under Race type, choose a training type. For your first run, choose Time trial. The agent with the default sensor configuration with a single-lens camera is suitable for this type of racing without modifications.
On the Create model page, under Training algorithm and hyperparameters, choose the Soft Actor Critic (SAC) or Proximal Policy Optimization (PPO) algorithm. In the AWS DeepRacer console, SAC models must be trained in continuous action spaces. PPO models can be trained in either continuous or discrete action spaces.
On the Create model page, under Training algorithm and hyperparameters, use the default hyperparameter values as is. Later on, to improve training performance, expand the hyperparameters and experiment with modifying the default hyperparameter values.
On the Create model page, under Agent, choose The Original DeepRacer or The Original DeepRacer (continuous action space) for your first model. If you use Soft Actor Critic (SAC) as your training algorithm, we filter your cars so that you can conveniently choose from a selection of compatible continuous action space agents.
On the Create model page, choose Next.
On the Create model page, under Reward function, use the default reward function example as is for your first model. Later on, you can choose Reward function examples to select another example function and then choose Use code to accept the selected reward function.
On the Create model page, under Stop conditions, leave the default Maximum time value as is or set a new value to terminate the training job to help prevent long-running (and possible run-away) training jobs. When experimenting in the early phase of training, you should start with a small value for this parameter and then progressively train for longer amounts of time.
On the Create model page, choose Create model to start creating the model and provisioning the training job instance.
After the submission, watch your training job being initialized and then run. The initialization process takes about 6 minutes to change status from Initializing to In progress.
Watch the Reward graph and Simulation video stream to observe the progress of your training job. You can choose the refresh button next to Reward graph periodically to refresh the Reward graph until the training job is complete.
Note: The training job is running on the AWS Cloud, so you don't need to keep the AWS DeepRacer console open during training. However, you can come back to the console to check on your model at any point while the job is in progress.

Part 2: Inspect your reward graph to assess your training progress
As you train and evaluate your first model, you'll want to get a sense of its quality. To do this, inspect your reward graph.

Find the following on your reward graph:

Average reward
Average percentage completion (training)
Average percentage completion (evaluation)
Best model line
Reward primary y-axis
Percentage track completion secondary y-axis
Iteration x-axis
Review the solution to this exercise for ideas on how to interpret it.


3.
---------------------------------------


3.
---------------------------------------


3.
---------------------------------------









