date start : 21 april 2025


9.1 Meet Your Instructors
---------------------------------------
We’re happy to have you here to learn about Natural Language Processing with TensorFlow!

Jocelyn is a Developer Advocate for the TensorFlow team at Google, and started her tech career in AI.

Michael is a Curriculum Manager at Udacity, and first began using TensorFlow way back before v1 was released.



9.2 Introduction to Natural Language Processing
---------------------------------------
Natural Language Processing, or NLP for short, focuses on analyzing text and speech data. This can range from simple recognition (what words are in the given text/speech), to sentiment analysis (was a review positive or negative), and all the way to areas like text generation (creating novel song lyrics from scratch).

We’ll focus only on text in these lessons and not speech, but many of the same principles apply.

NLP got its start mostly on machine translation, where users often had to create strict, manual rules to go from one language to another. It has since morphed to be more machine learning-based, reliant on much larger datasets than the early methods were.

Quiz Question
Which of the below areas are related to Natural Language Processing?
* speech recognition
* text translation

Further Research
In these lessons, we'll focus on how to implement certain NLP methods in TensorFlow code without necessarily diving deep on the background to each topic. If you are interested in a deeper dive, you should check out either of the two Nanodegree programs below:

* Deep Learning Nanodegree(opens in a new tab)
* Natural Language Processing Nanodegree(opens in a new tab)

We will also include a handful of videos from these programs within the Recurrent Neural Networks lesson later on for additional information.



9.3 Lesson Outline
---------------------------------------
In the first lesson on Natural Language Processing with TensorFlow, we’ll focus on Tokenization and Embeddings, which will help convert input text into useful data for input into the neural network layers you’ve seen before.

In the second lesson, we’ll dive into Recurrent Neural Networks (such as the LSTMs you saw in the Time Series Analysis lesson) as well as Text Generation, which allows for the creation of new text.


9.4 Tokenizing Text
---------------------------------------



9.5 Colab: Tokenizing Text
---------------------------------------



9.6
---------------------------------------



9.7
---------------------------------------



9.8
---------------------------------------



9.9
---------------------------------------



9.10
---------------------------------------



9.11
---------------------------------------



9.12
---------------------------------------



9.13
---------------------------------------



9.14
---------------------------------------



9.15
---------------------------------------



9.16
---------------------------------------



9.17
---------------------------------------



9.18
---------------------------------------




