date start : 21 april 2025


9.1 Meet Your Instructors
---------------------------------------
We’re happy to have you here to learn about Natural Language Processing with TensorFlow!

Jocelyn is a Developer Advocate for the TensorFlow team at Google, and started her tech career in AI.

Michael is a Curriculum Manager at Udacity, and first began using TensorFlow way back before v1 was released.



9.2 Introduction to Natural Language Processing
---------------------------------------
Natural Language Processing, or NLP for short, focuses on analyzing text and speech data. This can range from simple recognition (what words are in the given text/speech), to sentiment analysis (was a review positive or negative), and all the way to areas like text generation (creating novel song lyrics from scratch).

We’ll focus only on text in these lessons and not speech, but many of the same principles apply.

NLP got its start mostly on machine translation, where users often had to create strict, manual rules to go from one language to another. It has since morphed to be more machine learning-based, reliant on much larger datasets than the early methods were.

Quiz Question
Which of the below areas are related to Natural Language Processing?
* speech recognition
* text translation

Further Research
In these lessons, we'll focus on how to implement certain NLP methods in TensorFlow code without necessarily diving deep on the background to each topic. If you are interested in a deeper dive, you should check out either of the two Nanodegree programs below:

* Deep Learning Nanodegree(opens in a new tab)
* Natural Language Processing Nanodegree(opens in a new tab)

We will also include a handful of videos from these programs within the Recurrent Neural Networks lesson later on for additional information.



9.3 Lesson Outline
---------------------------------------
In the first lesson on Natural Language Processing with TensorFlow, we’ll focus on Tokenization and Embeddings, which will help convert input text into useful data for input into the neural network layers you’ve seen before.

In the second lesson, we’ll dive into Recurrent Neural Networks (such as the LSTMs you saw in the Time Series Analysis lesson) as well as Text Generation, which allows for the creation of new text.


9.4 Tokenizing Text
---------------------------------------
Neural networks utilize numbers as their inputs, so we need to convert our input text into numbers. Tokenization is the process of assigning numbers to our inputs, but there is more than one way to do this - should each letter have its own numerical token, each word, phrase, etc.

As you saw in the video, tokenizing based on letters with our current neural networks doesn’t always work so well - anagrams, for instance, may be made up of the same letters but have vastly different meanings. So, in our case, we’ll start by tokenizing each individual word.

Tokenizer
With TensorFlow, this is done easily through use of a Tokenizer, found within tf.keras.preprocessing.text. If you wanted only the first 10 most common words, you could initialize it like so:

tokenizer = Tokenizer(num_words=10)

Fit on Texts
Then, to fit the tokenizer to your inputs (in the below case a list of strings called sentences), you use .fit_on_texts():

tokenizer.fit_on_texts(sentences)

Text to Sequences
From there, you can use the tokenizer to convert sentences into tokenized sequences:

tokenizer.texts_to_sequences(sentences)

Out of Vocabulary Words
However, new sentences may have new words that the tokenizer was not fit on. By default, the tokenizer will just ignore these words and not include them in the tokenized sequences. However, you can also add an “out of vocabulary”, or OOV, token to represent these words. This has to be specified when originally creating the Tokenizer object.

tokenizer = Tokenizer(num_words=20, oov_token=’OOV’)

Viewing the Word Index
Lastly, if you want to see how the tokenizer has mapped numbers to words, use the tokenizer.word_index property to see this mapping.


silent  listen
burned  burden
infests fitness
looped  pooled


[1,2,3,4]

[1,5,6,4,7,1,8]

Further Research
Many NLP models get trained on very large text corpuses to avoid having too many OOV words. Below are a couple of great resources for finding datasets that you might find useful on your NLP journey:

* A popular Github repo(opens in a new tab) for NLP datasets https://github.com/niderhoff/nlp-datasets
* Google's newly public dataset search https://github.com/niderhoff/nlp-datasets


9.5 Colab: Tokenizing Text
---------------------------------------
https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c01_nlp_turn_words_into_tokens.ipynb#scrollTo=Fir7qd6X8eZc


9.6 Text to Sequences
---------------------------------------
Even after converting sentences to numerical values, there’s still an issue of providing equal length inputs to our neural networks - not every sentence will be the same length!

There’s two main ways you can process the input sentences to achieve this - padding the shorter sentences with zeroes, and truncating some of the longer sequences to be shorter. In fact, you’ll likely use some combination of these.

With TensorFlow, the pad_sequences function from tf.keras.preprocessing.sequence can be used for both of these tasks. Given a list of sequences, you can specify a maxlen (where any sequences longer than that will be cut shorter), as well as whether to pad and truncate from either the beginning or ending, depending on pre or post settings for the padding and truncating arguments. By default, padding and truncation will happen from the beginning of the sequence, so set these to post if you want it to occur at the end of the sequence.

If you wanted to pad and truncate from the beginning, you could use the following:

padded = pad_sequences(sequences, maxlen=10)

Further Research
Head here(opens in a new tab) if you’d like to check out the full TensorFlow documentation for pad_sequences.

https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences


9.7 Colab: Preparing Text to Use with Tensorflow Models
---------------------------------------
Colab Notebook
To access the Colab Notebook, login to your Google account and click on the link below:

Preparing Text to Use with TensorFlow Models

https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c02_nlp_padding.ipynb


9.8 Tokenization of Large Datasets
---------------------------------------

Everything you have learned previously applies fairly similarly to larger datasets. In many cases, you’ll want to be even more focused on the total number of words used with the Tokenizer, as well as understanding the right sequence length to create from pad_sequences.

In the upcoming Colab, we’ll use portions of a Sentiment Analysis Dataset on Kaggle(opens in a new tab) that contains both Amazon product and Yelp restaurant reviews.

Quiz Question
When specifying num_words in the Tokenizer, if set at 1,000:

Only the most common 1,000 words are kept.

Further Research
Once again, it can be useful to check out Google’s Dataset Search(opens in a new tab) to find large datasets to work with, and Kaggle also has a wide variety of datasets(opens in a new tab) available for use.


9.9
---------------------------------------



9.10
---------------------------------------



9.11
---------------------------------------



9.12
---------------------------------------



9.13
---------------------------------------



9.14
---------------------------------------



9.15
---------------------------------------



9.16
---------------------------------------



9.17
---------------------------------------



9.18
---------------------------------------




