date start : 27 june 2025


10.1 Recurrent Neural Networks Intro
---------------------------------------
In the previous lesson on Natural Language Processing with TensorFlow, we focused on Tokenization and Embeddings, which helped convert text into useful data for input into neural networks. However, these networks were not yet able to consider the actual sequence of the words in the input.

In this second lesson, we’ll dive into Recurrent Neural Networks (such as the LSTMs you saw in the Time Series Analysis lesson) as well as Text Generation, which allows for the creation of new text.



10.2 Basics of RNNs
---------------------------------------
Recurrent Neural Networks (RNNs) still take in some input x and output some y, but they also feed some of the output of the network back into itself. This may be done over and over, so that with text input, the network has some memory of words that came much earlier in a sequence.


Quiz Question
How do recurrent neural networks (RNNs) differ from the network architectures you’ve been working with previously?

RNNs utilize or store information regarding previous inputs to better understand sequences.


We’ve included some additional videos below that are optional for you to view and include a deeper dive into RNNs. They come from Udacity’s Deep Learning Nanodegree(opens in a new tab) program, and concern the history of RNNs, and two parts on more RNN basics.

(Optional) History of RNNs

(Optional) RNN Basics Part 1

(Optional) RNN Basics Part 2


10.3 Sentence Context and LSTMs
---------------------------------------
Simple RNNs are not always enough when working with text data. Longer sequences, such as a paragraph, often are difficult to handle, as the simple RNN structure loses information about previous inputs fairly quickly.

Long Short-Term Memory models, or LSTMs, help resolve this by keeping a “cell state” across time. These include a “forget gate”, where the cell can choose whether to keep or forget certain words to carry forward in the sequence.

Another interesting aspect of LSTMs is that they can be bidirectional, meaning that information can be passed both forward (later in the text sequence) and backward (earlier in the text sequence).

Quiz Question
For which of the following sentences would an LSTM likely perform better on than a vanilla RNN, if it needed to figure out both the sentiment, and the subject of that sentiment?

While watching the movie, I could only think of how terrible it was.

Once again, we’ve included some additional videos below that are optional for you to view and include a deeper dive into LSTMs. They come from Udacity’s Deep Learning Nanodegree(opens in a new tab) program, and concern both LSTM basics and architectures.

(Optional) LSTM Basics

(Optional) LSTM Architectures


10.4 Constructing LSTMs in Code
---------------------------------------
The code for an LSTM layer itself is just the LSTM layer from tf.keras.layers, with the number of LSTM cells to use. However, this is typically wrapped within a Bidirectional layer to make use of passing information both forward and backward in the network, as we noted on the previous page.

# A bidirectional LSTM layer with 64 nodes
tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))

One thing to note when using a Bidirectional layer is when you look at the model summary, if you put in 64 LSTM nodes, you will actually see a layer shape with 128 nodes (64x2).

No Need to Flatten
Unlike our more vanilla neural networks in the last lesson, you no longer need to use Flatten or GlobalAveragePooling1D after the LSTM layer - the LSTM can take the output of an Embedding layer and directly hook up to a fully-connected Dense layer with its own output.

Doubling Up
You can also feed an LSTM layer into another LSTM layer. To do so, on top of just stacking them in order when you create the model, you also need to set return_sequences to True for the earlier LSTM layer - otherwise, as noted above, the output will be ready for fully-connected layers and not be in the sequence format the LSTM layer expects.

# Two bidirectional LSTM layers with 64 nodes each
tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64), return_sequences=True)
tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))

Quiz Question
Which of the below is valid TensorFlow code for a LSTM layer?

tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))

Additional Research

You can check out the TensorFlow 2.0 documentation for LSTM layers here.

https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM


10.
---------------------------------------



10.
---------------------------------------




10.
---------------------------------------





