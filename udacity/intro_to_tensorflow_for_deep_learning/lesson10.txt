date start : 27 june 2025


10.1 Recurrent Neural Networks Intro
---------------------------------------
In the previous lesson on Natural Language Processing with TensorFlow, we focused on Tokenization and Embeddings, which helped convert text into useful data for input into neural networks. However, these networks were not yet able to consider the actual sequence of the words in the input.

In this second lesson, we’ll dive into Recurrent Neural Networks (such as the LSTMs you saw in the Time Series Analysis lesson) as well as Text Generation, which allows for the creation of new text.



10.2 Basics of RNNs
---------------------------------------
Recurrent Neural Networks (RNNs) still take in some input x and output some y, but they also feed some of the output of the network back into itself. This may be done over and over, so that with text input, the network has some memory of words that came much earlier in a sequence.


Quiz Question
How do recurrent neural networks (RNNs) differ from the network architectures you’ve been working with previously?

RNNs utilize or store information regarding previous inputs to better understand sequences.


We’ve included some additional videos below that are optional for you to view and include a deeper dive into RNNs. They come from Udacity’s Deep Learning Nanodegree(opens in a new tab) program, and concern the history of RNNs, and two parts on more RNN basics.

(Optional) History of RNNs

(Optional) RNN Basics Part 1

(Optional) RNN Basics Part 2


10.3 
---------------------------------------


10.
---------------------------------------


10.
---------------------------------------



10.
---------------------------------------




10.
---------------------------------------





